%%%%%%%%%%%%%%%%%%%%%%%
%% Author: Conghao Wong
%% Date: 2021-07-19 19:16:52
%% LastEditors: Conghao Wong
%% LastEditTime: 2021-08-30 20:26:55
%% Description: file content
%% Github: https://github.com/conghaowoooong
%% Copyright 2021 Conghao Wong, All Rights Reserved.
%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[../paper.tex]{subfiles}

\begin{document}
    
\section{\MODEL}

\subsection{Problem Settings}

Given a video clip $V = \{I_t\}$ that contains $N$ agents' observed trajectories during some period, the trajectory prediction is to forecast their future coordinate sequences according to their previous positions and the visual information of the scene.
Let $p_t = ({p_x}_t, {p_y}_t)$ denote some agent's 2D coordinate at step $t$ in the scene.
We denote his observation sequence among $t_h$ observed steps as $x = (p_1, p_2, ..., p_{t_h})$, and the future trajectory sequences in $t_f$ steps as $\hat{y} = (p_{t_h + 1}, p_{t_h + 2}, ..., p_{t_h + t_f})$.
The prediction task is to find as many as possible future predictions $\{\hat{y}\}$ for each observed agent in the future.
Formally, it aims at finding a network $f$, such that $\{\hat{y}\} = f(x, V)$.

\subsection{Method Overview}

\subfile{_fig_overview.tex}

This paper proposes the \MODEL, an encoder-decoder structure-based prediction network, to give agents multiple acceptable predictions in crowd spaces.
Unlike most previous methods, we employ the Discrete Fourier Transform (DFT) when encoding agents' trajectories.
It helps the network learn agents' behavior patterns that are hard to show in the time sequences.

\FIG{fig_overview} shows the \MODEL~main structure.
It contains two main designs, the \ALPHAMODEL~and \BETAMODEL.
Both of these two sub-networks have the encoder-decoder structure.

\subfile{_tab_method.tex}

\subsection{Keypoints Prediction}

The coarse \ALPHAMODEL~aims to forecast positions on several key future time steps.
In other words, it is committed to predicting the trajectory with \emph{a lower time and space resolution}.
It encodes all inputs (i.e., the observed trajectory and the context map) into the latent vector with a specific distribution.
Then, it samples vectors randomly from the distribution to generative multiple keypoints predictions.

\subsubsection{Embedding}

We first embed trajectory sequences and context maps before inputting them to the \ALPHAMODEL~encoder.
We apply the 1D-DFT on each dimension in trajectory sequences $x = \left\{(p_{x_t}, p_{y_t})\right\}_t$ to obtain the amplitude sequences $\{a_x, a_y\}$ and phase sequences $\{\phi_x, \phi_y\}$.
Formally,
\begin{equation}
    \begin{aligned}
        \mathcal{X} &= \mbox{DFT}[(p_{x_1}, p_{x_2}, ..., p_{x_{t_h}})], \\
        \mathcal{Y} &= \mbox{DFT}[(p_{y_1}, p_{y_2}, ..., p_{y_{t_h}})], \\
        a_x &= \Vert \mathcal{X} \Vert,~~a_y = \Vert \mathcal{Y} \Vert, \\
        \phi_x &= \arg \mathcal{X},~~\phi_y = \arg \mathcal{Y}.
    \end{aligned}
\end{equation}
We employ a trajectory embedding layer (see detailed structure in \TABLE{tab_method}) to embed agents' observed trajectories into the high-dimension
\begin{equation}
    f_t = \mbox{TE}([a_x, a_y, \phi_x, \phi_y]),
\end{equation}
where the $[a, b]$ represents the concatenation of $a$ and $b$.

Similarly, we guide a context embedding layer to embed the scene context map $C \in \mathbb{R}^{100 \times 100}$ into the
\begin{equation}
    f_c = \mbox{CE}(C),
\end{equation}
and then combine it with the above trajectory representation to obtain the embeded vector
\begin{equation}
    f_e = [f_t, f_c] \in \mathbb{R}^{t_h \times 128}.
\end{equation}

\subsubsection{Multi-Style Feature Encoding}

We use a Transformer-based Encoder to encode the input sequences.
The embeded vector $f_e$ is passed to the Transformer Encoder \cite{attentionIsAllYouNeed}, and the amplitude sequences $\{a_x, a_y\}$ and the phase sequences $\{\phi_x, \phi_y\}$ are passed to the Transformer decoder \cite{attentionIsAllYouNeed}.
Note that we only pickle outputs (marked with $f_{tra}$) from the penultimate layer of Transformer decoders (whose shapes are $t_h \times 128$), rather than the last output layer.

\cite{wong2021msn} guides a convolution layer to generate features with multiple styles in different output channels, therefore giving multiple deterministic predictions for the same agent at the same time.
Inspired by them, we employ the graph convolution operation to aggregate features on different time (frequency) nodes in feature sequences $f_{tra}$, and use them to represent $K_c$ styles of activity features $f_{K_c}$.
Formally,
\begin{equation}
    \begin{aligned}
        f_{K_c} &= (f_1, ..., f_k, ..., f_{K_c})^T \\
        &= \mbox{GraphConv}(f_{tra}, A) \in \mathbb{R}^{K_c \times 128},
    \end{aligned}
\end{equation}
where $A \in \mathbb{R}^{K_c \times t_h}$ is the trainable adjacency matrix.

\subsubsection{Multi-Modal Keypoints Prediction}

The \ALPHAMODEL~focuses on forecasting trajectories with a lower time resolution.
In other words, it concerns more about the establishment of agents' overall behavior styles rather than those fine interaction details.
We archive this goal by predicting positions on several ``key'' future steps.
We employ an MLP (called the keypoints decoder, KD) to predict coordinates on the key future steps.
Let $N_{key}$ be the number of these keypoints, we have the $k$th style keypoints prediction $\hat{y}_k^{key}$:
\begin{equation}
    \label{eq_keypointsprediction}
    \begin{aligned}
        (a_{xk}^{key}, a_{yk}^{key}, \phi_{xk}^{key}, \phi_{yk}^{key}) &= \mbox{KD}(f_k) \in \mathbb{R}^{N_{key} \times 4}, \\
        \mathcal{X}^{key} &= a_{xk}^{key} \exp(j\phi_{xk}^{key}), \\
        \mathcal{Y}^{key} &= a_{yk}^{key} \exp(j\phi_{yk}^{key}), \\
        \hat{y}_k^{key} &= \left(\mbox{IDFT}[\mathcal{X}^{key}],~\mbox{IDFT}[\mathcal{Y}^{key}]\right).
    \end{aligned}
\end{equation}

Besides, we apply the KL loss on each style's latent feature $f_k$.
It forces these features distributed similar enough to the normalized Gaussian distribution.
Thus, the keypoints decoder could output multi-modal generative predictions by inputting the randomly sampled noise vector $z \sim \mathcal{N}(0, I)$.
It makes the predictions in line with agents' uncertainty of plans.

\subsection{Frequency Domain Interpolation}

Although we can quickly obtain coordinates on all future prediction moments by applying the \emph{piecewise linear interpolation} on keypoints sequences, it may lose rich interaction details.
To address this problem, we propose the fine \BETAMODEL~to reconstruct the prediction from keypoints sequences.
Note that although \ALPHAMODEL~and \BETAMODEL~may use the same layer listed in \TABLE{tab_method}, they do not share the same weights.

We also apply the 1D-DFT on each dimension of trajectory sequences to let the \BETAMODEL~learns the differences between actual trajectories' amplitude and phase sequences and the predicted keypoints sequences.

For the amplitude and phase sequences from \EQUA{eq_keypointsprediction}, we have
the trajectory representation
\begin{equation}
    \label{eq_keydft}
    f^{key}_t = \mbox{TE}([a_{xk}^{key}, a_{yk}^{key}, \phi_{xk}^{key}, \phi_{yk}^{key}]).
\end{equation}
When training the \BETAMODEL, we apply the DFT on agents' groundtruth future trajectories $[x, y]$ to obtain the corresponding amplitude and phase sequences.
Formally,
\begin{equation}
    \begin{aligned}
        (\mathcal{X}^{gt}, \mathcal{Y}^{gt}) &= \mbox{DFT}[[x, y]], \\
        \mbox{where}~\mathcal{X}^{gt} &= a_{x}^{gt} \exp (j\phi_{x}^{gt}), \\
        \mathcal{Y}^{gt} &= a_{y}^{gt} \exp (j\phi_{y}^{gt}).
    \end{aligned}
\end{equation}
Similar to \EQUA{eq_keydft}, we have the groundtruth trajectories' representation:
\begin{equation}
    f^{gt}_t = \mbox{TE}([a_{x}^{gt}, a_{y}^{gt}, \phi_{x}^{gt}, \phi_{y}^{gt}]).
\end{equation}

To let the \BETAMODEL~learn the relationship between the keypoints sequences given by \ALPHAMODEL~and their realistic trajectories, we employ the similar Transformer-based encoder-decoder network (called the Interpolation, IP) here.
We pass the $f_e^{key} = [f_t^{key}, \mbox{CE}(C)]$ to the transformer encoder, and the amplitude and phase sequences $[a_{xk}^{key}, a_{yk}^{key}, \phi_{xk}^{key}, \phi_{yk}^{key}]$ to the transformer decoder.
The encoder outputs the predicted amplitude and phase sequences.
Formally,
\begin{equation}
    [\hat{a}_x, \hat{a}_y, \hat{\phi}_x, \hat{\phi}_y] = \mbox{IP}(f_e^{key}, [a_{xk}^{key}, a_{yk}^{key}, \phi_{xk}^{key}, \phi_{yk}^{key}]).
\end{equation}

The IDFT is applyed to obtain the entired predicted time-sequences $\hat{y}_o$:
\begin{equation}
    \begin{aligned}
        \hat{\mathcal{X}} &= \hat{a}_x \exp(\hat{\phi}_x), \\
        \hat{\mathcal{Y}} &= \hat{a}_y \exp(\hat{\phi}_y), \\
        \hat{y}_o &= \left(\mbox{IDFT}[\mathcal{X}^{key}],~\mbox{IDFT}[\mathcal{Y}^{key}]\right).
    \end{aligned}
\end{equation}
The $\hat{y}_o \in \mathbb{R}^{(t_h + t_f) \times 2}$ contains both the reconstructed observed trajectory and the predicted trajectory.
Finally, we have one of the \MODEL~outputs
\begin{equation}
    \hat{y} = \hat{y}_o [t_h:, :],
\end{equation}
where the $[t_h:, :]$ indicates the slicing operation.

\subsection{Loss Functions}

We use the below loss function to tain each part of the \MODEL:
\begin{equation}
    \begin{aligned}
        \mathcal{L} =& \mu_1 \underbrace{\Vert \hat{y}^{key} - y^{key} \Vert_2}_{\mbox{AKL}} + \mu_2 \underbrace{\mbox{KL}(P(f_k) \Vert \mathcal{N}(0, I))}_{\mbox{KL Loss}} + \\
        &\mu_3 \underbrace{\Vert \hat{y} - y \Vert_2}_{\mbox{APL}} + \mu_4 \underbrace{\Vert E(\hat{y}) - E(y) \Vert_2}_{\mbox{AEL}},
    \end{aligned}  
\end{equation}
where $\{\mu_1, \mu_2, \mu_3, \mu_4\}$ is a set of balance parameters, and the KL Loss term is used for training the encoder-decoder structure in \ALPHAMODEL~to generative multiple predictions.
Other loss function terms are listed below:

(a) \textbf{AKL} indicates the Average Keypoints Loss.
It is the average point-wise $\ell_2$ distance among keypoints and their predictions.
It is used to train variables in \ALPHAMODEL.

(b) \textbf{APL} indicates the Average Point-wise Loss.
It is the average point-wise $\ell_2$ distance among the entire future trajectory and the prediction.
We use this loss function to train variables in \BETAMODEL.

(c) \textbf{AEL} indicates the Average Energy Loss.
It is the average absolute interaction energy difference on predicted positions and their groundtruth positions.
Let $g = G(p)$ denote the coordinate transformation function that transfers real-world positions to the grid positions on the scene context maps, we have:
\begin{equation}
    E(y) = \sum_{p \in y}C[G(p)],
\end{equation}
where $C$ indicates the context map.
This loss function is used to train all variables in \MODEL~to make all predictions in line with social rules and physical constraints.

\end{document}